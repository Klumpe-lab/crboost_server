This is a cryoet job orchestrator project.
Currently a few jobs work and succeed in the pipeline, but the pipelines can be pretty long and each job takes time so it would be nice to be able to "come back" to a project where a few jobs have succeeded and not start from scratch. What do you think?

The general data flow is that parameters are collected by the job model from the global state (populated either from the defaults or via the ui -- you don't need concern yourself with that too much, it works). When the user begins the pipeline, the schemes are created and qsub slurm job script is copied from the config directory. Then the pipeline begins via the "pipeline_runner", which initiates the relion pipeline by invoking relion schemer in the appropriate container. Then relion dispatches one job after anohter by means of looking up their job_star's fn_exe and seeing that we have put the appropriate python "driver" there, substituting the string of this driver invocation into the XXXcommandXXX in our qsub slurm script and submitting this slurm script to the queue. Lastly, the given job's driver is executed on the compute node and knows how to call the appropriate tool (whether containerized or not -- right now all our tools are containerized) to do the heavy computational workload and then after that possibly some metadata processing for the results of that job or some other housekeeping defined in the driver. Once the driver is done -- it created a relion sentinel file (one of RELION_JOB_EXIT_SUCCESS or RELION_JOB_EXIT_FAILURE) and return the appropriate exit code whereby the slurm job ends and the control returns back to relion_schemer which, if the last job suceeded, creates the next as described above.

Let me sketch out the application state management for you and try to explain how it _should_ work, and then i'll show you the actual code we have which is subpar.

This is a cryoet job orchestrator project. The user specifies the primary data (usuall some frames and mdocs, later possibly a location for a gain reference file or a template or whatever). Using these files the backend infers some parameters about the the data. The user then is free to select from a number of predefined job types to construct a processing pipeline. Jobs (like fsMotionAndCtf, alignTilts etc.) leverage containerized tools from the cryoem/et ecosystem and the processing is unified under the purview of `relion_schemer` which tracks the status of individual jobs, updates the state of the pipeline and submits new jobs to the cluster whenever appropraite. Each job type is defined by the following entities:

- a relion-style `job.star` file with certain default parameters for the given job
- a pydantic model in the backend which specifies the types of these parameters, their transformations and the asset paths for this given job type (what are the necessary inputs for this job as well as what the job produces).
- a number of auxilary metadata transformations that the containerized tools themselves don't performm but are nonetheless necessary for the success of a given pipeline
- a "driver" file which is the bootrstrap mechanism that gets dispatched to the compute nodes of a slurm cluster and is able to collect all the necessary assets and parameters and execute the job per se.